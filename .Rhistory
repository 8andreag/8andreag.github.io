library(ggplot2)
# Create a grid of PC1 and PC2 values
grid <- expand.grid(
PC1 = seq(min(pca_scores$PC1), max(pca_scores$PC1), length.out = 100),
PC2 = seq(min(pca_scores$PC2), max(pca_scores$PC2), length.out = 100)
)
# Predict class for each point on the grid
grid$predicted <- predict(multinom_model, newdata = grid)
ggplot() +
geom_tile(data = grid, aes(x = PC1, y = PC2, fill = predicted), alpha = 0.3) +
geom_point(data = pca_scores, aes(x = PC1, y = PC2, color = G3_category), size = 2) +
scale_fill_manual(values = c("low" = "red", "medium" = "gray", "high" = "lightgreen")) +
scale_color_manual(values = c("low" = "red", "medium" = "gray", "high" = "ligtgreen")) +
labs(title = "Multinomial Logistic Regression Decision Boundaries",
subtitle = "Predicted Grade Categories in PC1 vs. PC2 Space",
x = "PC1", y = "PC2", fill = "Predicted", color = "Actual") +
theme_minimal()
ggplot() +
geom_tile(data = grid, aes(x = PC1, y = PC2, fill = predicted), alpha = 0.3) +
geom_point(data = pca_scores, aes(x = PC1, y = PC2, color = G3_category), size = 2) +
scale_fill_manual(values = c("low" = "red", "medium" = "gray", "high" = "lightgreen")) +
scale_color_manual(values = c("low" = "red", "medium" = "gray", "high" = "lightgreen")) +
labs(title = "Multinomial Logistic Regression Decision Boundaries",
subtitle = "Predicted Grade Categories in PC1 vs. PC2 Space",
x = "PC1", y = "PC2", fill = "Predicted", color = "Actual")
# Categorize G3 in the test set
student_test <- student_test |>
mutate(G3_category = case_when(
G3 < 10 ~ "low",
G3 >= 10 & G3 < 15 ~ "medium",
G3 >= 15 ~ "high"
))
# Prepare test predictors (same variables used in PCA)
test_predictors <- student_test |> select(-G3, -G3_category)
# Use the same PCA model to transform the test data
test_pca_scores <- predict(student_pca_model, newdata = test_predictors)
student_pca_recipe <- recipe(G3_category ~ ., data = student_train) |>
update_role(G3, new_role = "id") |>
step_dummy(all_nominal_predictors()) |>
step_center(all_numeric_predictors()) |>
step_scale(all_numeric_predictors())
G3_category
here::i_am("437proj.qmd")
#| label: load packages
#| message: false
#| warning: false
library(here)
library(readr)
library(dplyr)
library(ggplot2)
library(rsample)
library(tidymodels)
library(tidyverse)
library(recipes) # don't need the rest of tidymodels
library(naniar)
library(softImpute)
library(broom)
library(mice)
#| label: import data
#| warning: false
data <- readr::read_csv(here::here("student-por.csv"))
data
vis_miss(data[,1:33]) # default row and column order
student <- data |>
select(Pstatus, Fedu, Medu, guardian, studytime, famsup, G1, G2, G3)
student <- student |>
mutate(
Pstatus = as.factor(Pstatus),
guardian = as.factor(guardian),
studytime = as.factor(studytime),
famsup = as.factor(famsup)
)
student
set.seed(123)
student_split <- initial_split(student, prop = 0.80)  #splitting data  (random sample, 80%)
student_train <- training(student_split)
student_test <- testing(student_split)
student_train |>
miss_var_summary()
table(student_train$famsup)
ggplot(data = student_train,
mapping = aes(
x = famsup,
fill = famsup
)
) +
geom_bar() +
labs(title = "Distribution of Family Support", x = "Family Support", y = "Count")
student_G123 <- student_train |>
pivot_longer(cols = c(G1, G2, G3),
names_to = "Term",
values_to = "Grade")
ggplot(student_G123,
mapping = aes(
x = famsup,
y = Grade,
fill = famsup
)
) +
geom_boxplot() +
facet_wrap(~ Term) +
labs(title = "Grade Distribution by Family Support Across Terms",
x = "Family Support",
y = "Grade")
means_by_group <- student_G123 |>
group_by(Term, famsup) |>
summarise(mean_grade = mean(Grade, na.rm = TRUE), .groups = "drop")
means_by_group
ggplot(data = student_G123,
mapping = aes(
x = Grade,
fill = famsup)
) +
geom_histogram(binwidth = 1) +
facet_grid(
rows = vars(famsup), cols = vars(Term)
)
student_train |>
group_by(Fedu) |>
summarise(mean_G3 = mean(G3, na.rm = TRUE),
count = n())
ggplot(data = student_G123,
mapping = aes(
x = Fedu,
fill = famsup
)
) +
geom_histogram(binwidth = 1) +
labs(title = "Distribution of Father's Education", x = "Education Level", y = "Count")
student_train |>
group_by(Medu) |>
summarise(mean_G3 = mean(G3, na.rm = TRUE),
count = n())
ggplot(data = student_G123,
mapping = aes(
x = Medu,
fill = famsup)
) +
geom_histogram(binwidth = 1) +
labs(title = "Distribution of Mother's Education", x = "Education Level", y = "Count")
ggplot(student_G123,
mapping = aes(
x = Fedu,
y = Grade,
color = famsup
)
) +
geom_point(alpha = 0.6) +
geom_smooth(method = "lm", se = FALSE, color = "blue") +
facet_wrap(~ Term) +
labs(title = "Father's Education vs. Final Grades",
x = "Father's Education Level",
y = "Final Grades")
ggplot(student_G123,
mapping = aes(
x = Medu,
y = Grade,
color = famsup
)
) +
geom_point(alpha = 0.6) +
geom_smooth(method = "lm", se = FALSE, color = "red") +
facet_wrap(~ Term) +
labs(title = "Mother's Education vs. Final Grades",
x = "Mother's Education Level",
y = "Final Grades")
ggplot(data = student_G123,
mapping = aes(
x = studytime,
y = Grade,
fill = Pstatus
)
) +
geom_boxplot() +
facet_wrap(~ Term) +
labs(title = "Grade Distribution by Study Times Across Terms",
x = "Study Time",
y = "Grade")
student_G123 |>
group_by(Pstatus, Term) |>
summarise(
num_total = n(),
min = min(Grade, na.rm = TRUE),
median = median(Grade, na.rm = TRUE),
max = max(Grade, na.rm = TRUE),
mean = mean(Grade, na.rm = TRUE),
sd = sd(Grade, na.rm = TRUE)
)
student_train |>
group_by(guardian) |>
summarise(mean_G3 = mean(G3, na.rm = TRUE),
count = n())
ggplot(student, aes(x = guardian, fill = guardian)) +
geom_bar() +
labs(title = "Primary Caregiver Distribution",
x = "Guardian", y = "Count")
library(ggplot2)
ggplot(student, aes(x = G1)) +
geom_histogram(binwidth = 1, fill = "pink", color = "black") +
labs(title = "Distribution of G1 (First Trimester Grades)", x = "G1", y = "Count")
ggplot(student, aes(x = G2)) +
geom_histogram(binwidth = 1, fill = "lavender", color = "black") +
labs(title = "Distribution of G2 (Second Trimester Grades)", x = "G2", y = "Count")
ggplot(student, aes(x = G3)) +
geom_histogram(binwidth = 1, fill = "lightblue", color = "black") +
labs(title = "Distribution of G3 (Final Grades)", x = "G3", y = "Count")
cor(student[, c("G1", "G2", "G3")])
# Create a recipe for preprocessing
student_pca_recipe <- recipe(G3 ~ ., data = student_train) |>
step_dummy(all_nominal_predictors()) |>
step_center(all_numeric_predictors()) |>
step_scale(all_numeric_predictors())
library(ggplot2)
# Create a grid of PC1 and PC2 values
grid <- expand.grid(
PC1 = seq(min(pca_scores$PC1), max(pca_scores$PC1), length.out = 100),
PC2 = seq(min(pca_scores$PC2), max(pca_scores$PC2), length.out = 100)
)
# Predict class for each point on the grid
grid$predicted <- predict(multinom_model, newdata = grid)
ggplot() +
geom_tile(data = grid, aes(x = PC1, y = PC2, fill = predicted), alpha = 0.3) +
geom_point(data = pca_scores, aes(x = PC1, y = PC2, color = G3_category), size = 2) +
scale_fill_manual(values = c("low" = "red", "medium" = "gray", "high" = "lightgreen")) +
scale_color_manual(values = c("low" = "red", "medium" = "gray", "high" = "lightgreen")) +
labs(title = "Multinomial Logistic Regression Decision Boundaries",
subtitle = "Predicted Grade Categories in PC1 vs. PC2 Space",
x = "PC1", y = "PC2", fill = "Predicted", color = "Actual")
# test set
student_test <- student_test |>
mutate(G3_category = case_when(
G3 < 10 ~ "low",
G3 >= 10 & G3 < 15 ~ "medium",
G3 >= 15 ~ "high"
))
student_pca_recipe <- recipe(G3_category ~ ., data = student_train) |>
update_role(G3, new_role = "id") |>
step_dummy(all_nominal_predictors()) |>
step_center(all_numeric_predictors()) |>
step_scale(all_numeric_predictors())
predicted <- predict(multinom_model, type = "class")
table(predicted, pca_scores$G3_category)
# Coefficients
summary(multinom_model)$coefficients
# Z-scores and p-values
z <- summary(multinom_model)$coefficients / summary(multinom_model)$standard.errors
p <- 2 * (1 - pnorm(abs(z)))
p
library(nnet)
# Make sure G3_category is a factor
pca_scores$G3_category <- as.factor(pca_scores$G3_category)
# Fit the model using PC1 and PC2 as predictors
multinom_model <- multinom(G3_category ~ PC1 + PC2, data = pca_scores)
summary(multinom_model)
pca_scores <- as.data.frame(student_pca_model$x[, 1:2])  # PC1 and PC2
pca_scores$G3_category <- student_multinom_data$G3_category
student_pca_model <- prcomp(student_multinom_data |> select(-G3, -G3_category), scale. = TRUE)
#categorizing
student_multinom_data <- student_train |>
mutate(G3_category = case_when(
G3 < 10 ~ "low",         # Failing or poor performance
G3 >= 10 & G3 < 15 ~ "medium",  # Average performance
G3 >= 15 ~ "high"        # High-performing students
))
student_pca_model <- prcomp(student_multinom_data |> select(-G3, -G3_category), scale. = TRUE)
pca_scores <- as.data.frame(student_pca_model$x[, 1:2])  # PC1 and PC2
pca_scores$G3_category <- student_multinom_data$G3_category
library(nnet)
# Make sure G3_category is a factor
pca_scores$G3_category <- as.factor(pca_scores$G3_category)
# Fit the model using PC1 and PC2 as predictors
multinom_model <- multinom(G3_category ~ PC1 + PC2, data = pca_scores)
summary(multinom_model)
# Coefficients
summary(multinom_model)$coefficients
# Z-scores and p-values
z <- summary(multinom_model)$coefficients / summary(multinom_model)$standard.errors
p <- 2 * (1 - pnorm(abs(z)))
p
predicted <- predict(multinom_model, type = "class")
table(predicted, pca_scores$G3_category)
library(ggplot2)
# Create a grid of PC1 and PC2 values
grid <- expand.grid(
PC1 = seq(min(pca_scores$PC1), max(pca_scores$PC1), length.out = 100),
PC2 = seq(min(pca_scores$PC2), max(pca_scores$PC2), length.out = 100)
)
# Predict class for each point on the grid
grid$predicted <- predict(multinom_model, newdata = grid)
here::i_am("437proj.qmd")
#| label: load packages
#| message: false
#| warning: false
library(here)
library(readr)
library(dplyr)
library(ggplot2)
library(rsample)
library(tidymodels)
library(tidyverse)
library(recipes) # don't need the rest of tidymodels
library(naniar)
library(softImpute)
library(broom)
library(mice)
#| label: import data
#| warning: false
data <- readr::read_csv(here::here("student-por.csv"))
data
vis_miss(data[,1:33]) # default row and column order
student <- data |>
select(Pstatus, Fedu, Medu, guardian, studytime, famsup, G1, G2, G3)
student <- student |>
mutate(
Pstatus = as.factor(Pstatus),
guardian = as.factor(guardian),
studytime = as.factor(studytime),
famsup = as.factor(famsup)
)
student
set.seed(123)
student_split <- initial_split(student, prop = 0.80)  #splitting data  (random sample, 80%)
student_train <- training(student_split)
student_test <- testing(student_split)
student_train |>
miss_var_summary()
table(student_train$famsup)
ggplot(data = student_train,
mapping = aes(
x = famsup,
fill = famsup
)
) +
geom_bar() +
labs(title = "Distribution of Family Support", x = "Family Support", y = "Count")
student_G123 <- student_train |>
pivot_longer(cols = c(G1, G2, G3),
names_to = "Term",
values_to = "Grade")
ggplot(student_G123,
mapping = aes(
x = famsup,
y = Grade,
fill = famsup
)
) +
geom_boxplot() +
facet_wrap(~ Term) +
labs(title = "Grade Distribution by Family Support Across Terms",
x = "Family Support",
y = "Grade")
means_by_group <- student_G123 |>
group_by(Term, famsup) |>
summarise(mean_grade = mean(Grade, na.rm = TRUE), .groups = "drop")
means_by_group
ggplot(data = student_G123,
mapping = aes(
x = Grade,
fill = famsup)
) +
geom_histogram(binwidth = 1) +
facet_grid(
rows = vars(famsup), cols = vars(Term)
)
student_train |>
group_by(Fedu) |>
summarise(mean_G3 = mean(G3, na.rm = TRUE),
count = n())
ggplot(data = student_G123,
mapping = aes(
x = Fedu,
fill = famsup
)
) +
geom_histogram(binwidth = 1) +
labs(title = "Distribution of Father's Education", x = "Education Level", y = "Count")
student_train |>
group_by(Medu) |>
summarise(mean_G3 = mean(G3, na.rm = TRUE),
count = n())
ggplot(data = student_G123,
mapping = aes(
x = Medu,
fill = famsup)
) +
geom_histogram(binwidth = 1) +
labs(title = "Distribution of Mother's Education", x = "Education Level", y = "Count")
ggplot(student_G123,
mapping = aes(
x = Fedu,
y = Grade,
color = famsup
)
) +
geom_point(alpha = 0.6) +
geom_smooth(method = "lm", se = FALSE, color = "blue") +
facet_wrap(~ Term) +
labs(title = "Father's Education vs. Final Grades",
x = "Father's Education Level",
y = "Final Grades")
ggplot(student_G123,
mapping = aes(
x = Medu,
y = Grade,
color = famsup
)
) +
geom_point(alpha = 0.6) +
geom_smooth(method = "lm", se = FALSE, color = "red") +
facet_wrap(~ Term) +
labs(title = "Mother's Education vs. Final Grades",
x = "Mother's Education Level",
y = "Final Grades")
ggplot(data = student_G123,
mapping = aes(
x = studytime,
y = Grade,
fill = Pstatus
)
) +
geom_boxplot() +
facet_wrap(~ Term) +
labs(title = "Grade Distribution by Study Times Across Terms",
x = "Study Time",
y = "Grade")
student_G123 |>
group_by(Pstatus, Term) |>
summarise(
num_total = n(),
min = min(Grade, na.rm = TRUE),
median = median(Grade, na.rm = TRUE),
max = max(Grade, na.rm = TRUE),
mean = mean(Grade, na.rm = TRUE),
sd = sd(Grade, na.rm = TRUE)
)
student_train |>
group_by(guardian) |>
summarise(mean_G3 = mean(G3, na.rm = TRUE),
count = n())
ggplot(student, aes(x = guardian, fill = guardian)) +
geom_bar() +
labs(title = "Primary Caregiver Distribution",
x = "Guardian", y = "Count")
library(ggplot2)
ggplot(student, aes(x = G1)) +
geom_histogram(binwidth = 1, fill = "pink", color = "black") +
labs(title = "Distribution of G1 (First Trimester Grades)", x = "G1", y = "Count")
ggplot(student, aes(x = G2)) +
geom_histogram(binwidth = 1, fill = "lavender", color = "black") +
labs(title = "Distribution of G2 (Second Trimester Grades)", x = "G2", y = "Count")
ggplot(student, aes(x = G3)) +
geom_histogram(binwidth = 1, fill = "lightblue", color = "black") +
labs(title = "Distribution of G3 (Final Grades)", x = "G3", y = "Count")
cor(student[, c("G1", "G2", "G3")])
# Create a recipe for preprocessing
student_pca_recipe <- recipe(G3 ~ ., data = student_train) |>
step_dummy(all_nominal_predictors()) |>
step_center(all_numeric_predictors()) |>
step_scale(all_numeric_predictors())
library(ggplot2)
# Create a grid of PC1 and PC2 values
grid <- expand.grid(
PC1 = seq(min(pca_scores$PC1), max(pca_scores$PC1), length.out = 100),
PC2 = seq(min(pca_scores$PC2), max(pca_scores$PC2), length.out = 100)
)
# Predict class for each point on the grid
grid$predicted <- predict(multinom_model, newdata = grid)
ggplot() +
geom_tile(data = grid, aes(x = PC1, y = PC2, fill = predicted), alpha = 0.3) +
geom_point(data = pca_scores, aes(x = PC1, y = PC2, color = G3_category), size = 2) +
scale_fill_manual(values = c("low" = "red", "medium" = "gray", "high" = "lightgreen")) +
scale_color_manual(values = c("low" = "red", "medium" = "gray", "high" = "lightgreen")) +
labs(title = "Multinomial Logistic Regression Decision Boundaries",
subtitle = "Predicted Grade Categories in PC1 vs. PC2 Space",
x = "PC1", y = "PC2", fill = "Predicted", color = "Actual")
# test set
student_test <- student_test |>
mutate(G3_category = case_when(
G3 < 10 ~ "low",
G3 >= 10 & G3 < 15 ~ "medium",
G3 >= 15 ~ "high"
))
student_pca_recipe <- recipe(G3_category ~ ., data = student_train) |>
update_role(G3, new_role = "id") |>
step_dummy(all_nominal_predictors()) |>
step_center(all_numeric_predictors()) |>
step_scale(all_numeric_predictors())
#categorizing
student_multinom_data <- student_train |>
mutate(G3_category = case_when(
G3 < 10 ~ "low",         # Failing or poor performance
G3 >= 10 & G3 < 15 ~ "medium",  # Average performance
G3 >= 15 ~ "high"        # High-performing students
))
pca_scores <- as.data.frame(student_pca_model$x[, 1:2])  # PC1 and PC2
pca_scores$G3_category <- student_multinom_data$G3_category
library(nnet)
# Make sure G3_category is a factor
pca_scores$G3_category <- as.factor(pca_scores$G3_category)
# Fit the model using PC1 and PC2 as predictors
multinom_model <- multinom(G3_category ~ PC1 + PC2, data = pca_scores)
summary(multinom_model)
predicted <- predict(multinom_model, type = "class")
table(predicted, pca_scores$G3_category)
library(ggplot2)
# Create a grid of PC1 and PC2 values
grid <- expand.grid(
PC1 = seq(min(pca_scores$PC1), max(pca_scores$PC1), length.out = 100),
PC2 = seq(min(pca_scores$PC2), max(pca_scores$PC2), length.out = 100)
)
# Predict class for each point on the grid
grid$predicted <- predict(multinom_model, newdata = grid)
